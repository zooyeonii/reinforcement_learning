{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from random import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjacency matrix\n",
    "def adjacency_matrix(n):\n",
    "    a = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j :\n",
    "                a[i,j] = 0\n",
    "            elif i<j :\n",
    "                a[i,j] = randint(0,1)\n",
    "            else:\n",
    "                a[i,j] = a[j,i]\n",
    "    return a \n",
    "\n",
    "def rand_func(n, lower, upper):\n",
    "    return np.random.randint(lower, upper, size = n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. adjacency matrix = \n",
      " [[0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0.]]\n",
      "connection = \n",
      " [4. 3. 7. 5. 4. 7. 8. 6. 7. 5. 5. 5.]\n",
      "2. number of publish = \n",
      " [37 12 72  9 75  5 79 64 16  1 76 71]\n",
      "3. number of subscribe = \n",
      " [ 6 25 50 20 18 84 11 28 29 14 50 68]\n",
      "4. neighborhood =\n",
      " [21. 21. 42. 30. 24. 42. 45. 34. 40. 31. 28. 30.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "#0. number of clients\n",
    "num_c = 12\n",
    "\n",
    "#1. adjacency matrix / connection\n",
    "A = adjacency_matrix (num_c)\n",
    "connection = np.sum(A, axis=1)\n",
    "\n",
    "#2. number of publish\n",
    "num_pub = rand_func(num_c, 0, 100)\n",
    "\n",
    "#3. number of subscribe\n",
    "num_sub = rand_func(num_c, 0, 100)\n",
    "\n",
    "#4. neighborhood\n",
    "def neighborhood():\n",
    "    neighborhood = np.zeros(num_c)\n",
    "    for i in range(num_c):\n",
    "        for j in range(num_c):\n",
    "            if A[i,j]==1 :\n",
    "                neighborhood[i]+=connection[j]\n",
    "    return neighborhood\n",
    "\n",
    "print(\"1. adjacency matrix = \\n\",A)\n",
    "print(\"connection = \\n\",connection)\n",
    "print(\"2. number of publish = \\n\", num_pub)\n",
    "print(\"3. number of subscribe = \\n\", num_sub)\n",
    "print(\"4. neighborhood =\\n\", neighborhood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rargmax(vector):\n",
    "    # amax는 최고 값을 가지는 index를 무작위로 선택한다. \n",
    "    m = np.amax(vector)\n",
    "    indices = np.nonzero(vector == m)[0]\n",
    "    return random.choice(indices)\n",
    "def sum_pub_minus_sub(v):\n",
    "    sum_pub =0; sum_sub =0\n",
    "    seq = sorted(v);\n",
    "    for i in range(len(num_pub)):\n",
    "        for j in range(len(seq)):\n",
    "            if i==seq[j]:\n",
    "                sum_pub+=num_pub[i]\n",
    "                sum_sub+=num_sub[i]            \n",
    "    return sum_pub-sum_sub\n",
    "\n",
    "def sum_connection(v):\n",
    "    sum_conn = 0\n",
    "    seq = sorted(v)\n",
    "    for i in range(len(connection)):\n",
    "        for j in range(len(seq)):\n",
    "            if i==seq[j]:\n",
    "                sum_conn += connection[i]\n",
    "    return sum_conn\n",
    "def sum_neighbor(v):\n",
    "    sum_ne =0\n",
    "    neighbor = neighborhood()\n",
    "    seq = sorted(v)\n",
    "    for i in range(len(neighbor)):\n",
    "        for j in range(len(seq)):\n",
    "            if i==seq[j]:\n",
    "                sum_ne += neighbor[i]\n",
    "    return sum_ne\n",
    "#sum_neighbor([0,1])\n",
    "#sum_pub_minus_sub([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = num_c   # number of active peer\n",
    "action_size = 3 # +1 / 0 / -1\n",
    "num_episodes = 3\n",
    "rList=[]  # step 별 reward 저장 \n",
    "Q = np.zeros([state_size, action_size])\n",
    "max_reward = [] #lists to contain total rewards \n",
    "active_peer_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Episode  1 -------\n",
      "step:  0  active peer list : [11]\n",
      "reward : step은 0 리워드는, 21.88\n",
      "step 0 에서 Q는 [[21.88  0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]]\n",
      "new_state는  -1\n",
      "step:  1  active peer list : [9]\n",
      "reward : step은 1 리워드는, -97.95\n",
      "step 1 에서 Q는 [[ 21.88   0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.   -97.95   0.  ]]\n",
      "new_state는  -1\n",
      "-----Episode  2 -------\n",
      "step:  0  active peer list : [3]\n",
      "reward : step은 0 리워드는, -80.21\n",
      "step 0 에서 Q는 [[-80.21   0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.     0.     0.  ]\n",
      " [  0.   -97.95   0.  ]]\n",
      "new_state는  -1\n",
      "step:  1  active peer list : [8]\n",
      "reward : step은 1 리워드는, -151.67\n",
      "step 1 에서 Q는 [[ -80.21    0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.    -97.95 -151.67]]\n",
      "new_state는  0\n",
      "step:  2  active peer list : [11]\n",
      "reward : step은 2 리워드는, 15.62\n",
      "step 2 에서 Q는 [[ -80.21    0.     15.62]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.    -97.95 -151.67]]\n",
      "new_state는  1\n",
      "step:  3  active peer list : [2]\n",
      "reward : step은 3 리워드는, 224.58\n",
      "step 3 에서 Q는 [[ -80.21    0.     15.62]\n",
      " [   0.    224.58    0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.    -97.95 -151.67]]\n",
      "new_state는  1\n",
      "step:  4  active peer list : [3]\n",
      "reward : step은 4 리워드는, -57.29\n",
      "step 4 에서 Q는 [[ -80.21    0.     15.62]\n",
      " [   0.    167.29    0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.    -97.95 -151.67]]\n",
      "new_state는  1\n",
      "-----Episode  3 -------\n",
      "step:  0  active peer list : [11]\n",
      "reward : step은 0 리워드는, 15.62\n",
      "step 0 에서 Q는 [[ -80.21    0.    182.91]\n",
      " [   0.    167.29    0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.    -97.95 -151.67]]\n",
      "new_state는  1\n",
      "step:  1  active peer list : [2]\n",
      "reward : step은 1 리워드는, 224.58\n",
      "step 1 에서 Q는 [[ -80.21    0.    182.91]\n",
      " [   0.    391.87    0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.    -97.95 -151.67]]\n",
      "new_state는  1\n",
      "step:  2  active peer list : [3]\n",
      "reward : step은 2 리워드는, -57.29\n",
      "step 2 에서 Q는 [[ -80.21    0.    182.91]\n",
      " [   0.    334.58    0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.    -97.95 -151.67]]\n",
      "new_state는  1\n",
      "step:  3  active peer list : [1]\n",
      "reward : step은 3 리워드는, -28.44\n",
      "step 3 에서 Q는 [[ -80.21    0.    182.91]\n",
      " [   0.    306.14    0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.    -97.95 -151.67]]\n",
      "new_state는  1\n",
      "step:  4  active peer list : [6]\n",
      "reward : step은 4 리워드는, 850.0\n",
      "step 4 에서 Q는 [[ -80.21    0.    182.91]\n",
      " [   0.   1156.14    0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.      0.      0.  ]\n",
      " [   0.    -97.95 -151.67]]\n",
      "new_state는  1\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_episodes):\n",
    "    #1. 환경을 초기화하고, 첫 state를 가져온다.\n",
    "    state = 0\n",
    "    rAll = 0\n",
    "    done = False \n",
    "    max_steps = 5\n",
    "    step =0\n",
    "    # Q-table Learning algorithm\n",
    "    print(\"-----Episode \", i+1, \"-------\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        reward = 0\n",
    "        keep_stay = 0 # counting step\n",
    "        action = rargmax(Q[state,:])\n",
    "        active_peer_list = []\n",
    "            \n",
    "            \n",
    "        # Get new state and reward from environment\n",
    "        if action == 0 : \n",
    "            keep_stay = 0 \n",
    "            if state < 0 :\n",
    "                new_state = state\n",
    "                reward = -10000000\n",
    "                done = True\n",
    "            else :\n",
    "                new_state = state-1\n",
    "        elif action ==1:\n",
    "            new_state = state\n",
    "            keep_stay +=1\n",
    "        else:\n",
    "            keep_stay = 0\n",
    "            if state > num_c :\n",
    "                new_sate = state\n",
    "                reward = -1000000\n",
    "                done = True\n",
    "            else :\n",
    "                new_state = state +1\n",
    "                    \n",
    "        if done == True:\n",
    "            break\n",
    "        #select #(state) of active peers (print the peers' number)        \n",
    "        active_peer_list.append(random.randint(0,num_c-1))\n",
    "        print(\"step: \", step, \" active peer list :\", active_peer_list)\n",
    "            \n",
    "        #evaluate reward by those peers! \n",
    "            #(50-num_active)+(sum_publish-sum_subscribe)*(sum_connection/num_c)*(sum_neighborhood/num_c)\n",
    "        reward = (num_c/2-new_state)+(sum_pub_minus_sub(active_peer_list))*(sum_connection(active_peer_list)/num_c)*(sum_neighbor(active_peer_list)/num_c)\n",
    "        reward = round(reward,2)\n",
    "        print(\"reward : step은\", step,\"리워드는,\", reward)\n",
    "        #add list of active peers per \n",
    "        # Update Q-table \n",
    "        Q[state,action] = reward + np.max(Q[new_state,:])\n",
    "        print(\"step\",step,\"에서\",\"Q는\",Q)\n",
    "        rAll += reward\n",
    "        state = new_state\n",
    "        print(\"new_state는 \", new_state)\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
